#summary logstash configuration

= Table of Contents =

<wiki:toc max_depth="1" />

= Introduction =

The logstash config format is currently YAML. The basic structure of your
config will look like this:

{{{
inputs:
  < one or more inputs >
filters:
  < one or more filters >
outputs:
 < one or more outputs >
}}}

Filters are optional. If you don't need any filters in an agent, you can omit
the 'filters' section.

Inputs and filters use 'types' as an additional attribute. The 'type' should be
something semantically meaningful. A filter will only act on events that have
the same type as configured.

{{{
inputs:
  syslog:   # Define the 'syslog' type
  - /var/log/messages
filters:
- date:
    syslog:  # For 'syslog' type events, use the following date parsing...
      fieldname: "%Y/%m/%d %H:%M%S %Z"
}}}

*NOTE:* Inputs will only set the type for an event if there is no type already
set. For example, if an event received using the amqp input already has a type,
the type you gave to the amqp input it will be ignored and the previous value
kept. This ensures events that travel file to amqp to filter to elastic will
keep the original type through the entire pipeline.

= Inputs =

== file ==

Example: file:///var/log/*.log

File input. Uses 'eventmachine-tail' to tail the files. It will follow files
and react properly when files are truncated, rotated, or renamed. It also
supports globs, so if your application logs to an unpredictable filename (for
example, if it includes a timestamp or date), you can just include globs.

Currently, events are determined as file data delimited by a newline.

Multiline logs will be supported later. If you want this sooner, email the mailing list :)

== AMQP ==

Example: amqp://server:port/topic/somelogs

Like the AMQP output, this supports AMQP (a messaging protocol).  See the AMQP
output for more details.

Each AMQP message is treated like a logstash event and is expected to be in
JSON format.

== syslog ==

Example: syslog://0.0.0.0:5454/

Listens for syslog input on both TCP and UDP. It tries to obey RFC3164 where reasonable.
This is useful if you don't want to run a logstash agent on your other servers but already
have a syslog server capable of shipping logs out to other syslog servers.

= Filters =

== grok ==

Grok is one of the best ways to parse your plain-text logs into something
structured.  This filter requires you to have grok (library) and jls-grok
(rubygem) installed.

  * Grok: http://code.google.com/p/semicomplete/wiki/Grok

Example config:
{{{
filters;
- grok:
    syslog: # for logs of type 'linux-syslog'
      patterns:
      - %{SYSLOGLINE}
    apache-access: # for logs of type 'apache-error'
      patterns:
      - %{COMBINEDAPACHELOG}
    nagios:
      patterns:
      - %{NAGIOSLOGLINE}
}}}

Patterns for grok can be found in the logstash directory under 'patterns' or here: http://code.google.com/p/logstash/source/browse/trunk#trunk/patterns

== date ==

The date filter lets you parse a date from the event and use that as the timestamp for the event. If you do not use the date filter, or no date filter matches your event, the timestamp used for that event is the receive time of the event.

Example config:
{{{
- date:
    syslog:  # for logs of type 'syslog'
      # Look for a field 'timestamp' with this format, parse and it for the timestamp
      # This field comes from the SYSLOGLINE pattern
      timestamp: "%b %e %H:%M:%S"
      timestamp8601: ISO8601
    apache-access:
      timestamp: "%d/%b/%Y:%H:%M:%S %Z"
    nagios:
      epochtime: %s
}}}

The formats supported are "ISO8601" literally (see above) or any valid strftime
format. The 'key' in the configs above (timestamp, epochtime, etc) are fields from
your event that will be used to try and parse a time.

= Outputs =

== AMQP ==

Example: amqp://server:port/fanout/somequeue

AMQP output sends events to an AMQP message broker. You have 3 choices of
message queues: topic, fanout, or queue. A topic is a pubsub broadcast-type of
messaging. Fanout is  round-robin messaging. A queue is 1:1 messaging.

Example servers that support AMQP include: RabbitMQ, ActiveMQ, QPid.

AMQP is a great way to send events around from input collectors (file inputs)
to event indexers (elasticsearch output).

== stdout ==

Example: stdout:///

stdout is useful for outputting events you receive if you need to debug events
or want to use an agent that streams events from other inputs (files, amqp,
etc)

== ElasticSearch ==

Example: elasticsearch://server:port/logstash/events

ElasticSearch is the best way to store your logstash events for later searching.

Using the example url above will use the REST API supported by ElasticSearch.
This is an easy start to using ElasticSearch, but doesn't scale beyond a few
thousand qps (mostly due to current code features in logstash and use of
em-http-request).

=== Scaling ElasticSearch ===

If you want to scale better, you should use elasticsearch "river". This tells
elasticsearch to read indexing inputs from an AMQP message queue and should
scale better than the REST API. To use this, you should use something like this
as your output:

To use this river, you'll have to install the rabbitmq-river plugin for
elasticsearch. This is easy; from the elasticsearch install directory, run:

{{{
bin/plugin -install river-rabbitmq
}}}

It's called 'river-rabbitmq' but I'm pretty sure it is AMQP not specific to
rabbitmq.

Example URL:
elasticsearch://localhost:9200/logstash/events?method=river&type=rabbitmq&host=AMQPHOST&user=guest&pass=guest&vhost=/&queue=es

The url parameters are pretty straight forward, the ones you care about are:
  * host - the hostname of the AMQP broker
  * user - the amqp user
  * pass - the amqp password
  * vhost - the amqp vhost (if you don't know what this is, use '/')
  * queue - the queue to write to

== MongoDB ==

Example: mongodb://mongodbhost/dbname

This will write events to mongodb.
