#summary LogStash Design
#labels Phase-Design

= Introduction =

This page is an overview on the basic design of LogStash. It might help to be familiar with some LogStash [Terms].

= Logs =

Our goal is to be able to index any kind of log. We have initially provided facilities for indexing two different encodings of logs:

 * text-based
 * JSON-based

JSON logs might seem like an odd choice, but we end up turning everything into key-value pairs, so it's really the lowest common denominator.

When logs are defined, they may a set of user-defined attributes. These can be used when selecting which logs to search (vs. having to remember all of the different log types you defined).

Logs are made up of many "events". These log encodings happen to be line-based, but each line is really an "event". We will discuss what an "event" is composed of, and then discuss how we import both log formats.

== Events ==

Each event is made up of key-value pairs that describe the event, and some meta-data to go along with the event itself.

 * `@DATE`: a normalized (Unix time; seconds since the epoch) timestamp for the event
 * `@SOURCE_HOST`: the hostname that generated the log event
 * `@LINE`: the human-readable representation of the event

We index all of the key-value pairs and all of the meta-data except `@LINE`. `@LINE` is the only piece of data with an event that we actually _store_ in the database (but *not* index). This means that you will be searching for different `@LINE`s by the key-value pairs that produced it, not doing a full-text search on `@LINE` itself.

== Calculating `@DATE` ==

It is usually more desirable to figure out an event's true timestamp by extracting the date from the actual event than looking at the current time on the indexer when we receive the log-line (may have been queued, buffer when writing to the log, etc). LogStash allows you to define which key (field) represents the time, and a `strftime`-expression to match the date (we use `strptime` to parse the date).

You may have a log format without a date or based on some kind of real-time feed that doesn't have a date. In that case, LogStash will assign the current time as the event time.

== Text Logs ==

Text logs are traditionally hard to search because sometimes you have to write complex regular expressions (CPU intensive) to match a certain piece of the log. If we turn the log event into key-value pairs with [Grok], it becomes much simpler (and cheaper, CPU-wise) to search. [Grok] is an expert log processing tool (think very fancy regular expressions). One of it's output options is JSON, and we use that to turn an event that starts out as an opaque line of text into a more descriptive JSON object that describes the event.

For one log type, multiple grok patterns might be defined. The order is important, because once the first pattern matches, we go straight to indexing. In something complex like Linux syslog, you might define a few more specific patterns (better key-value pairs for a sudo log line, a pam sucess/failure, etc) and then fall back to the generic pattern (standard syslog line -- date, host, progname, message).

We then index the key-value pairs generated, and store the original text log as @LINE (the human-readable representation of the event).

For more details and examples, see [TextLogs].

== JSON Logs ==

JSON logs are very hard to search on disk without a specialized tool (i.e. first parse the JSON, and then search the fields). We hope that by making JSON logs easy to index that more programs will start having an option to log in JSON (easier to represent more complex data structures and events). There is not much parsing to do other than calculating the meta-data @DATE and @LINE. Unlike a text log, the original input (a JSON object in text form) isn't very human friendly. A JSON log definition comes with a `line_format` that is a Ruby `ERB` template string defining the human readable version of the event (@LINE).

For more details and example, see [JsonLogs].

= Components =

There are a few major components that make up LogStash. All the components talk the AMQP protocol to a central message broker (we recommend Rabbit MQ). The message broker allows us to easily scale and provide more reliability for delivering messages.

== Agent ==

The agent runs on every host you wish to collect logs from. It runs multiple threads (one per watched file) and sends new log lines to the parser (which eventually hit the indexer for actual indexing). The agent is "smart" in how we watch log files: automatically detects log-file roll-over (log rotation) and remembers file location for clean restarts without duplicate log lines indexed.

== Parser ==

The parser does the heavy lifting when indexing a new log line. For text logs, it applies Grok patterns to turn a line of text into JSON key-value pairs. For all logs, it calculates the meta-data `@DATE` (using strptime). After parsing a log line, the parsed value is sent to the indexer. The parser is part of `logstashd`. The parser can scale horizontally (recommended one per core). See also: [Scaling].

== Indexer ==

The indexer serves two purposes: indexing already-parsed log lines and searching. New log lines come in and are written to a Ferret/Lucene index and flushed to disk every ~30 seconds -- this provides fairly real-time indexing of new log lines. The indexer is the exclusive writer/reader for it's on-disk Ferret/Lucene index, so it also provides a search interface. The indexer is part of `logstashd`. The indexer can scale horizontally with each indexer having it's own on-disk index. See also: [Scaling].

== Search Web Interface ==

A small web interface (more proof of concept than anything) that provides an interface to searching. The query language is FQL (Ferret Query Language, see [QueryLanguage]). The output is always sorted by date (using the `@DATE` meta-data). There is also a command-line interface for log searching.

= Network Design =

See NetowrkDesign